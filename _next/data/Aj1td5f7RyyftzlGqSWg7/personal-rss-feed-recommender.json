{"pageProps":{"fileName":"Personal RSS feed recommender","file":"---\ndescription: Personal RSS feed recommender built using React and some machine learning\ndate: \"19 Feb 2023\"\ntags: [draft]\n---\n\n**Table of contents**\n- [Classifier](#classifier)\n\t- [Theory](#theory)\n\t- [Implementation](#implementation)\n- [Preprocessing](#preprocessing)\n\t- [Stopwords](#Stopwords)\n\t- [Lemmatisation](#lemmatisation)\n\nThis year I've decided to revive in myself the habit of reading. I enjoy reading posts from Habr, Medium, Quanta magazine, Stack Exchange (hot questions) and many more. I had a problem that these platforms are separate and I had to open them one by one to find something I like.\n\nThen, I remembered that sometime ago I used RSS feeds, they share up to date information from a feed that you can subscribe to. However, applications for these feeds did not filter or rank any content that went through, so, I would have huge piles posts that I would try to read through and most of the posts weren't that interesting. \n\nSo, I thought to make my own RSS feed, but with a classifier attached to it that predicts the probability that I will/won't like the post. \n\nTo keep the app simple, I've decided on a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for the ranker, [React](https://reactjs.org/) in [Typescript](https://www.typescriptlang.org/) for frontend and [Firebase](https://firebase.google.com/) for backend. \n\n## Classifier\n\nFirst I built the classifier to check if the idea would actually work. The idea behind naive Bayes classifier is to answer the question \"what is the probability that a given post $D$ belongs to a given class $C$ (like/dislike)?\" or if rephrased with probabilities: what is $p(C | D)$?\n\n### Theory\n\nIn the end, I  ended up using this formula for the probability:\n\n$$\np( ❤ | D) = 1 - \\frac{1}{1 + e ^ L}\n$$\nwhere $L$ is\n\n$$\nL = \\ln \\frac{p(❤)}{p(\\neg ❤)}  + \\sum_{w_i \\in D} \\ln \\frac{p(w_i | ❤)}{p(w_i | \\neg ❤)}.\n$$\n\nLooks scary, but actually it's not. Let's break it down.\n\nWe'll start off with making our first assumption, a document $D$ that is made up of words $w_i$ can be modeled as a bag of words that have independent distributions and that the $w_i$ appears in a document of class $C$ is $p(w_i | C)$. Then, we can claim that\n$$\n\\tag{1} p(D|C) = \\prod_{w_i \\in D} p(w_i | C).\n$$\n\nBayes' theorem states that\n$$\np(C | D) = \\frac{p(C) P(D | C)}{p(D)},\n$$\nand plugging in $(1)$ results in\n$$\n\\tag{2} p(C | D) = \\frac{p(C)}{p(D)} \\prod_{w_i \\in D} p(w_i | C).\n$$\nThis is the standard formula for a naive Bayes classifier, all you have to do is count up the frequencies, multiply, and compare what probability is greater $p(C_1 | D)$ or $p(C_2 | D)$? \n\nBut, I want to modify it. First, let's take the logarithm of the formula\n$$\n\\tag{3} \\ln p(C | D) = \\ln \\frac{p(C)}{p(D)} + \\sum_{w_i \\in D} \\ln p(w_i | C).\n$$\nTaking the logarithm of the formula will make sure we avoid problems with small floating points [^log_expl] and also it allows to use addition and subtraction rather than multiplication and division. \n\n[^log_expl]: This is because probabilities themselves work in the range $[0, 1]$, but if we take the logarithm of the probability then it works in range $(-\\infty, 0]$ giving  computer more space to work with and allowing it to be more accurate.\n\nSecond, instead of computing the probability, let's compute the likelihood ratio (ratio of the probabilities for like and not like):\n\n$$\n\\begin{align}\nL \n&= \\ln p(❤ | D) - \\ln p(\\neg ❤ | D) \\\\\n&= \\ln \\frac{p(❤ | D)}{p(\\neg ❤ | D)} \\\\\n&= \\ln \\frac{p(❤)}{p(\\neg ❤)}  + \\sum_{w_i \\in D} \\ln \\frac{p(w_i | ❤)}{p(w_i | \\neg ❤)}.\n\\end{align} \\tag{4}\n$$\nUsing that $p(❤ | D) + p(\\neg ❤ | D) = 1$ we get the probability of a like given a document:\n$$\n\\tag{5} p( ❤ | D) = 1 - \\frac{1}{1 + e ^ L}.\n$$\nWe got the formula from the beginning. [^L-expl] \n\n[^L-expl]: One might find it weird that I am taking a long way around to find the probability by introducing $L$ instead of using the formula $(3)$ derived earlier, however, there is a point to that. \n\t\n\tIn classical naive Bayes classifier implementations sometimes the probabilities are greater than one and don't even add up to one. This is because because _the assumption we made in the beginning was not true_.\n\t\n\tThe document does depend on the structure and ordering of the words. Disregarding that results in probabilities that are actually dependent and simply multiplying them will not cut. However, this doesn't make the classifier dysfunctional, even flawed it has proven itself working.\n\t\n\tBy introducing $L$ and deriving $p(❤ | D)$ from $p(❤ | D) + p(\\neg ❤ | D) = 1$ results in probabilities that add up to one.\n\n### Implementation\n\nFor the implementation I introduce a tiny constant value `eps`, so that in some cases I avoid division by zero.\n\n```typescript\nconst eps = 1e-4;\n\ntype Freq = {\n\tlike: number,\n\tdislike: number\n}\n\ntype Words = {\n    like: {\n        [word: string]: number\n    },\n    dislike: {\n        [word: string]: number\n    }\n}\n```\n\nFirst I make a function to compute the sum of an array.\n```typescript\nconst sum = (arr: number[]) => {\n\treturn arr.reduce((a, b) => a + b, eps);\n}\n```\n\nTo compute the probability of a class (prior), I count the frequency of a class in the data, divide it by the total number of occurrences and return the logarithm of it.\n\n```typescript\nconst log_prior = (\n\tc: \"like\" | \"dislike\", \n\tn_messages: Freq\n) => {\n\tconst vals = Object.values(n_messages);\n\tconst lp = Math.log(n_messages[c] / sum(vals));\n\t\n\treturn lp;\n}\n```\n\nSimilarly, I find the probability of a word in a class (likelihood) by counting the frequency the word appears in the class and dividing by the number of words in the class and return the logarithm of it.\n\n```typescript\nconst log_likelihood = (\n\tword: string, \n\tc: \"like\" | \"dislike\", \n\tn_words: Words, \n\tn_messages: Freq\n) => {\n\tif (n_words[c] && n_words[c].hasOwnProperty(word)) {\n\t\tconst n_word = n_words[c][word] || 0;\n\t\tconst n_c = n_messages[c];\n\t\t\n\t\treturn Math.log((n_word + eps) / (n_c + eps));\n\t}\n\telse {\n\t\treturn Math.log(eps);\n\t}\n}\n```\n\nFinally, we'll combine it all together to\n\n```javascript\nconst NaiveBayesClassifier = (\n\twords: string[], n_messages: Freq, n_words: Words\n): number => {\n\tconst log_p_like = log_prior(\"like\", n_messages);\n\tconst log_p_dislike = log_prior(\"dislike\", n_messages);\n\t\n\tconst log_p_words_like = sum(\n\t\twords.map(words => \n\t\t\tlog_likelihood(words, \"like\", n_words, n_messages)\n\t\t)\n\t);\n\t\n\tconst log_p_words_dislike = sum(\n\t\twords.map(words => \n\t\t\tlog_likelihood(words, \"dislike\", n_words, n_messages)\n\t\t)\n\t);\n\t\n\tconst log_p_like_words = log_p_like + log_p_words_like;\n\tconst log_p_dislike_words = log_p_dislike + log_p_words_dislike;\n\t\n\tconst L = log_p_like_words - log_p_dislike_words;\n\tconst approx_p = 1 - 1/(1 + Math.exp(L));\n\t\n\treturn approx_p;\n};\n```\n\n## Preprocessing\n\n### Stopwords\n\nOne way to improve the results of a naive Bayes classifier is to remove *stop-words* from the text. Stop-words are frequent words, like _\"a\"_ and _\"the\"_, that don't carry much meaning. This was done by filtering words from a list with ~ 100 words.\n\nI downloaded the stop-word lists from [NLTK](https://www.nltk.org/), and cleaned the texts before passing to the classifier.\n\n```typescript\nconst removeStopWords = (\n\twords: string[], \n\tstopwords: string[]\n) => {\n    return words.filter(word => !stopwords.includes(word));\n}\n```\n\n### Lemmatisation\n\nSimilarly, lemmatising words improves the classifier. I used the word maps from NLTK.\n\n```typescript\ntype Lemmatizer = (word: string) => string;\n\nconst lemmatize = (\n\twords: string[], \n\tlemmatizer: Lemmatizer\n) => {\n    return words.map(word => lemmatizer(word));\n}\n```\n\n"},"__N_SSG":true}