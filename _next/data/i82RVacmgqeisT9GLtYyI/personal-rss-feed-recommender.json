{"pageProps":{"fileName":"Personal RSS feed recommender","file":"---\ndescription: Personal RSS feed recommender built using React and some machine learning\ndate: \"19 Feb 2023\"\ntags:\n\t- draft\n---\n\nThis year I've decided to revive in myself the habit of reading. I enjoy reading posts from Habr, Medium, Quanta magazine, Stack Exchange (hot questions) and many more. I had a problem that these platforms are separate and I had to open them one by one to find something I like.\n\nThen, I remembered that sometime ago I used RSS feeds, they share up to date information from a feed that you can subscribe to. However, applications for these feeds did not filter or rank any content that went through, so, I would have huge piles posts that I would try to read through and most of the posts weren't that interesting. \n\nSo, I thought to make my own RSS feed, but with a classifier attached to it that predicts the probability that I will/won't like the post. \n\nTo keep the app simple, I've decided on a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for the ranker, [React](https://reactjs.org/) for frontend and [Firebase](https://firebase.google.com/) for backend. \n\n## Classifier\n\n### Theory\n\nFirst I built the classifier to check if the idea would actually work. The idea behind naive Bayes classifier is to answer the question \"what is the probability that a given post $D$ belongs to a given class $C$ (like/dislike)?\" or if rephrased with probabilities: what is $p(C | D)$?\n\nFor simplicity we'll  assume that the probability of a document $D$ is in a class $C$ is given by the product of probabilities that the words $w_i$ of $D$ are in $C$:\n\n$$\np(D|C) = \\prod_i p(w_i | C)\n$$\n\nUsing Bayes' theorem\n$$\np(C | D) = \\frac{p(C) P(D | C)}{p(D)}\n$$\nThen we get\n$$\np(C | D) = \\frac{p(C)}{p(D)} \\prod_i p(w_i | C)\n$$\nwhich we can compute using the frequencies of the words in classes and frequencies of classes.  \n\nThat formula is enough to compute the probability, however, we can improve it by \n- taking natural logarithm of each side to avoid problems with small floating points, \n- instead of computing the probability compute the likelihood ratio (ratio of the probabilities like and dislike)\n\n$$\nL = \\ln \\frac{p(❤️ | D)}{p(\\neg ❤️ | D)} = \\ln \\frac{p(❤️)}{p(\\neg ❤️)}  + \\sum_i \\ln \\frac{p(w_i | ❤️)}{p(w_i | \\neg ❤️)}\n$$\nDue to the assumptions made in the beginning in reality the probabilities won't add up to one ($p(❤️ | D) + p(\\neg ❤️ | D) \\not= 1$) because they are not correct. \n\nWe'll get the probability back from the $L$\n$$\np( ❤️ | D) = 1 - \\frac{1}{1 + e ^ L}\n$$\n\nIn code it would look something like this\n\n### Implementation\n\nFirst we'll make a function to compute the sum.\n```javascript\nconst sum = (arr: number[]) => {\n\treturn arr.reduce((a, b) => a + b, eps);\n}\n```\n\nTo compute the log prior, we'll find the frequency of a class and return the log of it.\n```javascript\nconst log_prior = (c: \"like\" | \"dislike\", n_messages: Freq) => {\n\tconst vals = Object.values(n_messages);\n\tconst lp = Math.log(n_messages[c] / sum(vals));\n\t\n\treturn lp;\n}\n```\n\nSimilarly, we'll find the frequency of a word in a class and return the log of it.\n\n```javascript\nconst log_likelihood = (word: string, c: c, n_words: Words, n_messages: Freq) => {\n\tif (n_words[c] && n_words[c].hasOwnProperty(word)) {\n\t\tconst n_word = n_words[c][word] || 0;\n\t\tconst n_c = n_messages[c];\n\t\t\n\t\treturn Math.log((n_word + eps) / (n_c + eps));\n\t}\n\telse {\n\t\treturn Math.log(eps);\n\t}\n}\n```\n\nFinally, we'll combine it all together to\n\n```javascript\nconst NaiveBayesClassifier = (\n\twords: string[], n_messages: Freq, n_words: Words\n): number => {\n\tconst log_p_like = log_prior(\"like\", n_messages);\n\tconst log_p_dislike = log_prior(\"dislike\", n_messages);\n\t\n\tconst log_p_words_like = sum(\n\t\twords.map(words => \n\t\t\tlog_likelihood(words, \"like\", n_words, n_messages)\n\t\t)\n\t);\n\t\n\tconst log_p_words_dislike = sum(\n\t\twords.map(words => \n\t\t\tlog_likelihood(words, \"dislike\", n_words, n_messages)\n\t\t)\n\t);\n\t\n\tconst log_p_like_words = log_p_like + log_p_words_like;\n\tconst log_p_dislike_words = log_p_dislike + log_p_words_dislike;\n\t\n\tconst L = log_p_like_words - log_p_dislike_words;\n\tconst approx_p = 1 - 1/(1 + Math.exp(L));\n\t\n\treturn approx_p;\n};\n```\n\n## Application\n\n### Frontend\n\n### Backend\n\n"},"__N_SSG":true}