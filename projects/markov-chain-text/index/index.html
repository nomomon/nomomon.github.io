<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/af35e6bd8f5544cf.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/71c2a5fd3536d403.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-e3fa942d0b9eb126.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-cc48c28d170fddc2.js" async="" crossorigin=""></script><script src="/_next/static/chunks/69-73e2843bcaf99c09.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-d5466ca1a63a2877.js" async="" crossorigin=""></script><script src="/_next/static/chunks/250-d4164d91bfac7d8a.js" async=""></script><script src="/_next/static/chunks/app/layout-19f1b3ed134323a6.js" async=""></script><title>Mansur Nurmukhambetov</title><meta name="description" content="Generated by create next app"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="__className_aaf875"><nav class="fixed top-0 h-12 w-full mb-4 shadow-sm flex items-center justify-center z-10 bg-white bg-opacity-[50] bg-blend-soft-light"><div class="w-full max-w-4xl flex gap-4 text-gray-500 max-md:px-4 max-md:max-w-full"><a class="duration-300 hover:text-blue-500 text-foreground font-semibold" href="/">nomomon</a><a class="duration-300 hover:text-blue-500" href="/">About</a><a class="duration-300 hover:text-blue-500" href="/blog">Blog</a><a class="duration-300 hover:text-blue-500" href="/projects">Projects</a></div></nav><main class="mt-2 m-auto max-w-4xl py-16 max-md:px-4 max-md:max-w-full"><div><div class="markdown-body"><article><p><img src="./assets/markov-chain-text.jpeg" alt="thumbnail"></p>
<h1>Markov Chain Text Generator</h1>
<p>Recently, I was reading in the Y.Practicum blog and found a <a href="https://thecode.media/markov-chain/">group of posts</a> there about Markov chains. The articles were written in a easy to understand and engaging way, and I thought it would be interesting to see for myself how the algorithm works.</p>
<p>First of all, the definition:</p>
<blockquote>
<p>Markov chains are a sequence of events or actions, where each new event depends only on the previous one and does not take into account all other events. Such an algorithm does not remember what happened before, but only looks at the previous state.</p>
</blockquote>
<p>It will be easier to understand if you look at the following example:</p>
<pre><code class="hljs language-javascript"><span class="hljs-keyword">const</span> dataset = [<span class="hljs-string">&quot;I am a human&quot;</span>, <span class="hljs-string">&quot;I am a programmer&quot;</span>, <span class="hljs-string">&quot;I am not a dog&quot;</span>];
</code></pre>
<p>This dataset results in the following tree diagram:</p>
<p><img src="./assets/diagram.png" alt=""></p>
<p>In context of text generation, each word is an event. The next word is selected randomly from the words that stood after the last word. The probability of each word transition is determined by the frequency of the pair. After a random word is selected, the same thing is repeated with the sentence until the final word appears.</p>
<p>In the example from the image, after the word “a” in the source text there could be the words “programmer”, “human”, and “dog”. So, the new sentences that can be generated from the dataset are:</p>
<blockquote>
<p>I am not a programmer</p>
<p>I am not a human</p>
<p>I am a dog</p>
</blockquote>
<p>It certainly doesn't compare to GPT-3 or Copilot, but the results are sure ridiculous.</p>
<h2>Results</h2>
<p>Example in russian</p>
<p><img src="https://github.com/nomomon/markov-chain-text/raw/master/screenshot.png" alt="demo"></p>
</article></div></div></main><script src="/_next/static/chunks/webpack-e3fa942d0b9eb126.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/af35e6bd8f5544cf.css\",\"style\",{\"crossOrigin\":\"\"}]\n3:HL[\"/_next/static/css/71c2a5fd3536d403.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L4\"\n"])</script><script>self.__next_f.push([1,"5:I[7690,[],\"\"]\n8:I[5613,[],\"\"]\na:I[1778,[],\"\"]\nb:I[9553,[\"250\",\"static/chunks/250-d4164d91bfac7d8a.js\",\"185\",\"static/chunks/app/layout-19f1b3ed134323a6.js\"],\"NavBar\"]\nd:I[8955,[],\"\"]\n9:[\"slug\",\"projects/markov-chain-text/index\",\"oc\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"4:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/af35e6bd8f5544cf.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/71c2a5fd3536d403.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"oWSkY-L6MxrsIOlFlhe4W\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/projects/markov-chain-text/index/\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"projects/markov-chain-text/index\",\"oc\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"projects\\\",\\\"markov-chain-text\\\",\\\"index\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"projects/markov-chain-text/index\",\"oc\"],{\"children\":[\"__PAGE__\",{},[\"$L6\",\"$L7\",null]]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$9\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_aaf875\",\"children\":[[\"$\",\"$Lb\",null,{}],[\"$\",\"main\",null,{\"className\":\"mt-2 m-auto max-w-4xl py-16 max-md:px-4 max-md:max-w-full\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}]]}]}],null]],\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"f:T832,"])</script><script>self.__next_f.push([1,"\u003cp\u003e\u003cimg src=\"./assets/markov-chain-text.jpeg\" alt=\"thumbnail\"\u003e\u003c/p\u003e\n\u003ch1\u003eMarkov Chain Text Generator\u003c/h1\u003e\n\u003cp\u003eRecently, I was reading in the Y.Practicum blog and found a \u003ca href=\"https://thecode.media/markov-chain/\"\u003egroup of posts\u003c/a\u003e there about Markov chains. The articles were written in a easy to understand and engaging way, and I thought it would be interesting to see for myself how the algorithm works.\u003c/p\u003e\n\u003cp\u003eFirst of all, the definition:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMarkov chains are a sequence of events or actions, where each new event depends only on the previous one and does not take into account all other events. Such an algorithm does not remember what happened before, but only looks at the previous state.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt will be easier to understand if you look at the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-javascript\"\u003e\u003cspan class=\"hljs-keyword\"\u003econst\u003c/span\u003e dataset = [\u003cspan class=\"hljs-string\"\u003e\u0026quot;I am a human\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;I am a programmer\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;I am not a dog\u0026quot;\u003c/span\u003e];\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis dataset results in the following tree diagram:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"./assets/diagram.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIn context of text generation, each word is an event. The next word is selected randomly from the words that stood after the last word. The probability of each word transition is determined by the frequency of the pair. After a random word is selected, the same thing is repeated with the sentence until the final word appears.\u003c/p\u003e\n\u003cp\u003eIn the example from the image, after the word “a” in the source text there could be the words “programmer”, “human”, and “dog”. So, the new sentences that can be generated from the dataset are:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI am not a programmer\u003c/p\u003e\n\u003cp\u003eI am not a human\u003c/p\u003e\n\u003cp\u003eI am a dog\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt certainly doesn't compare to GPT-3 or Copilot, but the results are sure ridiculous.\u003c/p\u003e\n\u003ch2\u003eResults\u003c/h2\u003e\n\u003cp\u003eExample in russian\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/nomomon/markov-chain-text/raw/master/screenshot.png\" alt=\"demo\"\u003e\u003c/p\u003e\n"])</script><script>self.__next_f.push([1,"7:[false,[\"$\",\"div\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"markdown-body\",\"children\":[\"$\",\"article\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$f\"}}]}]}]]\nc:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Mansur Nurmukhambetov\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Generated by create next app\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>