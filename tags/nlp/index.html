<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/0db310923dda8a34.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0db310923dda8a34.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-50116e63224baba2.js" defer=""></script><script src="/_next/static/chunks/main-95cf1e829bcb8407.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0e6b96157a72fee4.js" defer=""></script><script src="/_next/static/chunks/961-b93ac647cf18494c.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Btag%5D-b74a61aae79bcce7.js" defer=""></script><script src="/_next/static/jyWZ2U_4vJOMu3Cq5iJZd/_buildManifest.js" defer=""></script><script src="/_next/static/jyWZ2U_4vJOMu3Cq5iJZd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><article class="mt-16 min-h-full w-full max-w-screen-sm mx-auto"><h1 class="mb-8">#nlp</h1><div class="flex flex-row justify-end mb-8"><a class="ml-4 text-gray-500 hover:text-gray-600 underline" href="/">Home</a><a class="ml-4 text-gray-500 hover:text-gray-600 underline" href="/tags/project/">Projects</a><a class="ml-4 text-gray-500 hover:text-gray-600 underline" href="/tags/blog/">Blog</a><a class="ml-4 text-gray-500 hover:text-gray-600 underline" href="/tags/note/">Notes</a></div><ol class="list-disc"><div class="mb-6"><a href="/posts/bayes-feed/"><h3 class="mb-2">Bayes Feed</h3></a><p class="mb-2"><span class="mr-2">An RSS feed with a Naive Bayes recommender that is learning online.</span><a class="underline text-gray-500" href="/posts/bayes-feed/">Read more →</a></p><small class="text-gray-500">Sat, Dec 17, 2022</small></div><div class="mb-6"><a href="/posts/data-science-bootcamp-yandex-practicum/"><h3 class="mb-2">Data Science Bootcamp Yandex Practicum</h3></a><p class="mb-2"><span class="mr-2">Collection of final projects from data science bootcamp from Yandex Practicum.</span><a class="underline text-gray-500" href="/posts/data-science-bootcamp-yandex-practicum/">Read more →</a></p><small class="text-gray-500">Sun, Feb 27, 2022</small></div><div class="mb-6"><a href="/posts/markov-chain-text-generator/"><h3 class="mb-2">Markov Chain Text Generator</h3></a><p class="mb-2"><span class="mr-2">Text generator written in React.js that uses Markov chains to generate text based on a given input.</span><a class="underline text-gray-500" href="/posts/markov-chain-text-generator/">Read more →</a></p><small class="text-gray-500">Sat, Aug 6, 2022</small></div></ol></article><footer class="mt-16 mb-8 w-full max-w-screen-sm mx-auto"><p class="text-gray-500 text-xs">2019 - <!-- -->2023<!-- --> © </p></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"nlp","posts":[{"path":"public/Bayes Feed.md","publish":true,"title":"Bayes Feed","description":"An RSS feed with a Naive Bayes recommender that is learning online.","date":"17 Dec, 2022","tags":["next.js","firebase","material-ui","nlp","rss","vercel","project"],"content":"\n![[bayes-feed.jpeg]]\n\n[RSS feeds](https://ru.wikipedia.org/wiki/RSS) are a convenient and old–fashioned way to follow blogs and websites. \n\nThis December, I returned to reading as my main source of getting information. In doing so, I came across a recommendation system with a Naive Bayes classifier, which was used to filter an RSS feed. This inspired me to create a similar application for myself.\n\nThe idea of the project is to collect posts from RSS feeds, use the classifier to predict whether I will like the post or not, and then train the algorithm on my reactions. Hence, this algorithm is learning in [online mode](https://en.wikipedia.org/wiki/Online_machine_learning).\n\nAt first I used a Telegram bot as an interface, but I encountered a lot of errors and inconveniences when parsing messages. I also used PocketBase for the backend, but it turned out that it is still in beta and query functions I needed were not available. As the result, I changed the stack to Next.js with MUI for frontend and Firebase for backend.\n\nUpdate\n---\n\nWith the latest update, I've made it easier and more efficient for users to access and filter their news.\n\nOne of the major updates I've made is the addition of authentication. Now, users can create their own accounts and have all their data stored in a single object, making it easier to query and manage.\n\nI've also streamlined the process of retrieving data by making all necessary requests in a single request. This not only makes the process faster, but it also helps stay within the Firebase free limit. \n\nBut that's not all - I've also improved the functionality of our Naive Bayes classifier. Rather than simply classifying articles as either liked or not liked, it now considers the approximate probability of a user liking an article. This gives users a more personalized experience and helps them find the content that is most relevant to them.\n\nFinally, I've hosted the app on Vercel, a reliable and user-friendly platform. The only issue I've encountered is the limitation of 12 requests for server functions on the free plan. While I understand that paying the $20/month fee will make the app be perfect, I cannot afford that for a personal use project.\n\nOverall, I'm confident that my RSS feed application will provide users with a valuable and enjoyable experience. I hope you'll give it a try and see for yourself!","md":"\u003cp\u003e![[bayes-feed.jpeg]]\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ru.wikipedia.org/wiki/RSS\"\u003eRSS feeds\u003c/a\u003e are a convenient and old–fashioned way to follow blogs and websites.\u003c/p\u003e\n\u003cp\u003eThis December, I returned to reading as my main source of getting information. In doing so, I came across a recommendation system with a Naive Bayes classifier, which was used to filter an RSS feed. This inspired me to create a similar application for myself.\u003c/p\u003e\n\u003cp\u003eThe idea of the project is to collect posts from RSS feeds, use the classifier to predict whether I will like the post or not, and then train the algorithm on my reactions. Hence, this algorithm is learning in \u003ca href=\"https://en.wikipedia.org/wiki/Online_machine_learning\"\u003eonline mode\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAt first I used a Telegram bot as an interface, but I encountered a lot of errors and inconveniences when parsing messages. I also used PocketBase for the backend, but it turned out that it is still in beta and query functions I needed were not available. As the result, I changed the stack to Next.js with MUI for frontend and Firebase for backend.\u003c/p\u003e\n\u003ch2\u003eUpdate\u003c/h2\u003e\n\u003cp\u003eWith the latest update, I’ve made it easier and more efficient for users to access and filter their news.\u003c/p\u003e\n\u003cp\u003eOne of the major updates I’ve made is the addition of authentication. Now, users can create their own accounts and have all their data stored in a single object, making it easier to query and manage.\u003c/p\u003e\n\u003cp\u003eI’ve also streamlined the process of retrieving data by making all necessary requests in a single request. This not only makes the process faster, but it also helps stay within the Firebase free limit.\u003c/p\u003e\n\u003cp\u003eBut that’s not all - I’ve also improved the functionality of our Naive Bayes classifier. Rather than simply classifying articles as either liked or not liked, it now considers the approximate probability of a user liking an article. This gives users a more personalized experience and helps them find the content that is most relevant to them.\u003c/p\u003e\n\u003cp\u003eFinally, I’ve hosted the app on Vercel, a reliable and user-friendly platform. The only issue I’ve encountered is the limitation of 12 requests for server functions on the free plan. While I understand that paying the $20/month fee will make the app be perfect, I cannot afford that for a personal use project.\u003c/p\u003e\n\u003cp\u003eOverall, I’m confident that my RSS feed application will provide users with a valuable and enjoyable experience. I hope you’ll give it a try and see for yourself!\u003c/p\u003e\n"},{"path":"public/Data Science Bootcamp Yandex Practicum.md","publish":true,"title":"Data Science Bootcamp Yandex Practicum","description":"Collection of final projects from data science bootcamp from Yandex Practicum.","date":"27 Feb, 2022","tags":["python","numpy","pandas","sklearn","tensorflow","keras","compvis","nlp","bootcamp","project"],"content":"\n![[yandex-practicum.png]]\n\nYandex Practicum is a data science bootcamp that I attended in 2022. The bootcamp is a 9-month long program that covers the basics of data science and machine learning. The program is divided into 3 modules:\n\n-   Data Preprocessing,\n-   Data Analysis,\n-   Machine Learning.\n\nEach module consists of 3-4 projects that tackle close to real-world problems. The projects are evaluated by a team of mentors. The final project is a capstone project that is made for and evaluated by real companies.\n\n## Projects\n\n**The project solutions are not shared publicly to prevent plagiarism. To see my solutions, send me a message with your github email.**\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth\u003eProject\u003c/th\u003e\n        \u003cth\u003eDescription\u003c/th\u003e\n        \u003cth\u003eTools\u003c/th\u003e\n        \u003cth\u003eSource\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd\u003eExploratory Data Analysis\u003c/td\u003e\n        \u003ctd\u003eBlah blah\u003c/td\u003e\n        \u003ctd\u003ePython, Pandas, Matplotlib, Seaborn\u003c/td\u003e\n        \u003ctd\u003e\u003ca href=\"#\"\u003easd\u003c/a\u003e\u003c/td\u003e\n    \u003ctr/\u003e\n\u003c/table\u003e\n","md":"\u003cp\u003e![[yandex-practicum.png]]\u003c/p\u003e\n\u003cp\u003eYandex Practicum is a data science bootcamp that I attended in 2022. The bootcamp is a 9-month long program that covers the basics of data science and machine learning. The program is divided into 3 modules:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eData Preprocessing,\u003c/li\u003e\n\u003cli\u003eData Analysis,\u003c/li\u003e\n\u003cli\u003eMachine Learning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach module consists of 3-4 projects that tackle close to real-world problems. The projects are evaluated by a team of mentors. The final project is a capstone project that is made for and evaluated by real companies.\u003c/p\u003e\n\u003ch2\u003eProjects\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eThe project solutions are not shared publicly to prevent plagiarism. To see my solutions, send me a message with your github email.\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth\u003eProject\u003c/th\u003e\n        \u003cth\u003eDescription\u003c/th\u003e\n        \u003cth\u003eTools\u003c/th\u003e\n        \u003cth\u003eSource\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd\u003eExploratory Data Analysis\u003c/td\u003e\n        \u003ctd\u003eBlah blah\u003c/td\u003e\n        \u003ctd\u003ePython, Pandas, Matplotlib, Seaborn\u003c/td\u003e\n        \u003ctd\u003e\u003ca href=\"#\"\u003easd\u003c/a\u003e\u003c/td\u003e\n    \u003ctr/\u003e\n\u003c/table\u003e\n"},{"path":"public/Markov Chain Text Generator.md","publish":true,"title":"Markov Chain Text Generator","description":"Text generator written in React.js that uses Markov chains to generate text based on a given input.","date":"Aug 6, 2022","tags":["react","node.js","nlp","markov-chain","project"],"content":"\n![[markov-chain-text.jpeg]]\n\nRecently, I was reading in the Y.Practicum blog and found a [group of posts](https://thecode.media/markov-chain/) there about Markov chains. The articles were written in a easy to understand and engaging way, and I thought it would be interesting to see for myself how the algorithm works.\n\nFirst of all, the definition:\n\n\u003e Markov chains are a sequence of events or actions, where each new event depends only on the previous one and does not take into account all other events. Such an algorithm does not remember what happened before, but only looks at the previous state.\n\nIt will be easier to understand if you look at the following example:\n\n```javascript\nconst dataset = [\"I am a human\", \"I am a programmer\", \"I am not a dog\"];\n```\n\nThis dataset results in the following tree diagram:\n\n![[diagram.png]]\n\nIn context of text generation, each word is an event. The next word is selected randomly from the words that stood after the last word. The probability of each word transition is determined by the frequency of the pair. After a random word is selected, the same thing is repeated with the sentence until the final word appears.\n\nIn the example from the image, after the word “a” in the source text there could be the words “programmer”, “human”, and “dog”. So, the new sentences that can be generated from the dataset are:\n\n\u003e I am not a programmer\n\u003e\n\u003e I am not a human\n\u003e\n\u003e I am a dog\n\nIt certainly doesn't compare to GPT-3 or Copilot, but the results are sure ridiculous.\n\n## Results\n\nExample in Russian\n\n![demo](https://github.com/nomomon/markov-chain-text/raw/master/screenshot.png)\n","md":"\u003cp\u003e![[markov-chain-text.jpeg]]\u003c/p\u003e\n\u003cp\u003eRecently, I was reading in the Y.Practicum blog and found a \u003ca href=\"https://thecode.media/markov-chain/\"\u003egroup of posts\u003c/a\u003e there about Markov chains. The articles were written in a easy to understand and engaging way, and I thought it would be interesting to see for myself how the algorithm works.\u003c/p\u003e\n\u003cp\u003eFirst of all, the definition:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMarkov chains are a sequence of events or actions, where each new event depends only on the previous one and does not take into account all other events. Such an algorithm does not remember what happened before, but only looks at the previous state.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt will be easier to understand if you look at the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-javascript\"\u003e\u003cspan class=\"hljs-keyword\"\u003econst\u003c/span\u003e dataset = [\u003cspan class=\"hljs-string\"\u003e\u0026quot;I am a human\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;I am a programmer\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;I am not a dog\u0026quot;\u003c/span\u003e];\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis dataset results in the following tree diagram:\u003c/p\u003e\n\u003cp\u003e![[diagram.png]]\u003c/p\u003e\n\u003cp\u003eIn context of text generation, each word is an event. The next word is selected randomly from the words that stood after the last word. The probability of each word transition is determined by the frequency of the pair. After a random word is selected, the same thing is repeated with the sentence until the final word appears.\u003c/p\u003e\n\u003cp\u003eIn the example from the image, after the word “a” in the source text there could be the words “programmer”, “human”, and “dog”. So, the new sentences that can be generated from the dataset are:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI am not a programmer\u003c/p\u003e\n\u003cp\u003eI am not a human\u003c/p\u003e\n\u003cp\u003eI am a dog\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt certainly doesn’t compare to GPT-3 or Copilot, but the results are sure ridiculous.\u003c/p\u003e\n\u003ch2\u003eResults\u003c/h2\u003e\n\u003cp\u003eExample in Russian\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/nomomon/markov-chain-text/raw/master/screenshot.png\" alt=\"demo\"\u003e\u003c/p\u003e\n"}]},"__N_SSG":true},"page":"/tags/[tag]","query":{"tag":"nlp"},"buildId":"jyWZ2U_4vJOMu3Cq5iJZd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>